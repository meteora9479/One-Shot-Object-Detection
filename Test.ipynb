{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import pandas as pd\n",
    "\n",
    "import _init_paths\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import argparse\n",
    "import pprint\n",
    "import pdb\n",
    "import time\n",
    "import cv2\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pickle\n",
    "from roi_data_layer.roidb import combined_roidb\n",
    "from roi_data_layer.roibatchLoader import roibatchLoader\n",
    "from model.utils.config import cfg, cfg_from_file, cfg_from_list, get_output_dir\n",
    "from model.rpn.bbox_transform import clip_boxes\n",
    "# from model.nms.nms_wrapper import nms\n",
    "from model.roi_layers import nms\n",
    "from model.rpn.bbox_transform import bbox_transform_inv\n",
    "from model.utils.net_utils import save_net, load_net, vis_detections\n",
    "from model.faster_rcnn.vgg16 import vgg16\n",
    "from model.faster_rcnn.resnet import resnet\n",
    "\n",
    "import pdb\n",
    "\n",
    "try:\n",
    "    xrange          # Python 2\n",
    "except NameError:\n",
    "    xrange = range  # Python 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_weight(weight, time, seen):\n",
    "  time = np.where(time==0, 1, time)\n",
    "  weight = weight/time[:,np.newaxis]\n",
    "  result_map = np.zeros((len(weight), len(weight)))\n",
    "  for i in range(len(weight)):\n",
    "    for j in range(len(weight)):\n",
    "      v1 = weight[i]\n",
    "      v2 = weight[j]\n",
    "      # v1_ = np.linalg.norm(v1)\n",
    "      # v2_ = np.linalg.norm(v2)\n",
    "      # v12 = np.sum(v1*v2)\n",
    "      # print(v12)\n",
    "      # print(v1_)\n",
    "      # print(v2_)\n",
    "      distance = np.linalg.norm(v1-v2)\n",
    "      if np.sum(v1*v2)== 0 :\n",
    "        result_map[i][j] = 0\n",
    "      else:\n",
    "        result_map[i][j] = distance\n",
    "      \n",
    "\n",
    "  df = pd.DataFrame (result_map)\n",
    "\n",
    "  ## save to xlsx file\n",
    "\n",
    "  filepath = 'similarity_%d.xlsx'%(seen)\n",
    "\n",
    "  df.to_excel(filepath, index=False)\n",
    "\n",
    "  weight = weight*255\n",
    "\n",
    "\n",
    "  cv2.imwrite('./weight_%d.png'%(seen), weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using config:\n",
      "{'ANCHOR_RATIOS': [0.5, 1, 2],\n",
      " 'ANCHOR_SCALES': [4, 8, 16, 32],\n",
      " 'CROP_RESIZE_WITH_MAX_POOL': True,\n",
      " 'CUDA': False,\n",
      " 'DATA_DIR': '/workspace/data',\n",
      " 'DEDUP_BOXES': 0.0625,\n",
      " 'EPS': 1e-14,\n",
      " 'EXP_DIR': 'res50',\n",
      " 'FEAT_STRIDE': [16],\n",
      " 'GPU_ID': 0,\n",
      " 'MATLAB': 'matlab',\n",
      " 'MAX_NUM_GT_BOXES': 20,\n",
      " 'MOBILENET': {'DEPTH_MULTIPLIER': 1.0,\n",
      "               'FIXED_LAYERS': 5,\n",
      "               'REGU_DEPTH': False,\n",
      "               'WEIGHT_DECAY': 4e-05},\n",
      " 'PIXEL_MEANS': array([[[102.9801, 115.9465, 122.7717]]]),\n",
      " 'POOLING_MODE': 'align',\n",
      " 'POOLING_SIZE': 7,\n",
      " 'RESNET': {'FIXED_BLOCKS': 2, 'MAX_POOL': False},\n",
      " 'RNG_SEED': 3,\n",
      " 'ROOT_DIR': '/workspace/One-Shot-Object-Detection',\n",
      " 'TEST': {'BBOX_REG': True,\n",
      "          'HAS_RPN': True,\n",
      "          'MAX_SIZE': 1000,\n",
      "          'MODE': 'nms',\n",
      "          'NMS': 0.3,\n",
      "          'PROPOSAL_METHOD': 'gt',\n",
      "          'RPN_MIN_SIZE': 16,\n",
      "          'RPN_NMS_THRESH': 0.7,\n",
      "          'RPN_POST_NMS_TOP_N': 300,\n",
      "          'RPN_PRE_NMS_TOP_N': 6000,\n",
      "          'RPN_TOP_N': 5000,\n",
      "          'SCALES': [600],\n",
      "          'SVM': False},\n",
      " 'TRAIN': {'ASPECT_GROUPING': False,\n",
      "           'BATCH_SIZE': 128,\n",
      "           'BBOX_INSIDE_WEIGHTS': [1.0, 1.0, 1.0, 1.0],\n",
      "           'BBOX_NORMALIZE_MEANS': [0.0, 0.0, 0.0, 0.0],\n",
      "           'BBOX_NORMALIZE_STDS': [0.1, 0.1, 0.2, 0.2],\n",
      "           'BBOX_NORMALIZE_TARGETS': True,\n",
      "           'BBOX_NORMALIZE_TARGETS_PRECOMPUTED': True,\n",
      "           'BBOX_REG': True,\n",
      "           'BBOX_THRESH': 0.5,\n",
      "           'BG_THRESH_HI': 0.5,\n",
      "           'BG_THRESH_LO': 0.0,\n",
      "           'BIAS_DECAY': False,\n",
      "           'BN_TRAIN': False,\n",
      "           'DISPLAY': 20,\n",
      "           'DOUBLE_BIAS': False,\n",
      "           'FG_FRACTION': 0.25,\n",
      "           'FG_THRESH': 0.5,\n",
      "           'GAMMA': 0.1,\n",
      "           'HAS_RPN': True,\n",
      "           'IMS_PER_BATCH': 1,\n",
      "           'LEARNING_RATE': 0.001,\n",
      "           'MARGIN': -0.3,\n",
      "           'MAX_SIZE': 1000,\n",
      "           'MOMENTUM': 0.9,\n",
      "           'PROPOSAL_METHOD': 'gt',\n",
      "           'RPN_BATCHSIZE': 256,\n",
      "           'RPN_BBOX_INSIDE_WEIGHTS': [1.0, 1.0, 1.0, 1.0],\n",
      "           'RPN_CLOBBER_POSITIVES': False,\n",
      "           'RPN_FG_FRACTION': 0.5,\n",
      "           'RPN_MIN_SIZE': 8,\n",
      "           'RPN_NEGATIVE_OVERLAP': 0.3,\n",
      "           'RPN_NMS_THRESH': 0.7,\n",
      "           'RPN_POSITIVE_OVERLAP': 0.7,\n",
      "           'RPN_POSITIVE_WEIGHT': -1.0,\n",
      "           'RPN_POST_NMS_TOP_N': 2000,\n",
      "           'RPN_PRE_NMS_TOP_N': 12000,\n",
      "           'SCALES': [600],\n",
      "           'SNAPSHOT_ITERS': 5000,\n",
      "           'SNAPSHOT_KEPT': 3,\n",
      "           'SNAPSHOT_PREFIX': 'res50_faster_rcnn',\n",
      "           'STEPSIZE': [30000],\n",
      "           'SUMMARY_INTERVAL': 180,\n",
      "           'TRIM_HEIGHT': 600,\n",
      "           'TRIM_WIDTH': 600,\n",
      "           'TRUNCATED': False,\n",
      "           'USE_ALL_GT': True,\n",
      "           'USE_FLIPPED': False,\n",
      "           'USE_GT': False,\n",
      "           'WEIGHT_DECAY': 0.0001,\n",
      "           'query_size': 128},\n",
      " 'USE_GPU_NMS': True,\n",
      " 'test_categories': [1],\n",
      " 'train_categories': [1]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/One-Shot-Object-Detection/lib/model/utils/config.py:382: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  yaml_cfg = edict(yaml.load(f))\n"
     ]
    }
   ],
   "source": [
    "lr = cfg.TRAIN.LEARNING_RATE\n",
    "momentum = cfg.TRAIN.MOMENTUM\n",
    "weight_decay = cfg.TRAIN.WEIGHT_DECAY\n",
    "\n",
    "np.random.seed(cfg.RNG_SEED)\n",
    "\n",
    "cfg.TRAIN.USE_FLIPPED = False\n",
    "dataset_name='coco'\n",
    "#imdb_name = \"voc_2007_trainval\"\n",
    "imdb_name='coco_2014_train'\n",
    "#imdbval_name = \"voc_2007_test\"\n",
    "imdbval_name = 'coco_2014_minival'\n",
    "#set_cfgs = ['ANCHOR_SCALES', '[8, 16, 32]', 'ANCHOR_RATIOS', '[0.5,1,2]']\n",
    "set_cfgs = ['ANCHOR_SCALES', '[4, 8, 16, 32]', 'ANCHOR_RATIOS', '[0.5,1,2]']\n",
    "net_name='res50'\n",
    "group_name=1\n",
    "cfg_file = \"cfgs/{}_{}.yml\".format(net_name, group_name) if group_name != 0 else \"cfgs/{}.yml\".format(net_name)\n",
    "cfg_from_file(cfg_file)\n",
    "cfg_from_list(set_cfgs)\n",
    "print('Using config:')\n",
    "pprint.pprint(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.68s)\n",
      "creating index...\n",
      "index created!\n",
      "Loaded dataset `coco_2014_minival` for training\n",
      "Set proposal method: gt\n",
      "coco_2014_minival gt roidb loaded from /workspace/data/cache/coco_2014_minival_gt_roidb.pkl\n",
      "Preparing training data...\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "cfg.TRAIN.USE_FLIPPED = False\n",
    "imdb_vu, roidb_vu, ratio_list_vu, ratio_index_vu, query_vu = combined_roidb(imdbval_name, False, seen=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_vu.competition_mode(on=True)\n",
    "dataset_vu = roibatchLoader(roidb_vu, ratio_list_vu, ratio_index_vu, query_vu, 1, imdb_vu.num_classes, \n",
    "                            training=False, seen=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initilize the network here.\n",
    "class_agnostic=True\n",
    "fasterRCNN = resnet(imdb_vu.classes, 50, pretrained=False, class_agnostic=class_agnostic)\n",
    "fasterRCNN.create_architecture()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load checkpoint models/res50/coco/faster_rcnn_1_10_1663.pth\n"
     ]
    }
   ],
   "source": [
    "# Load checkpoint\n",
    "checksession_int=1\n",
    "checkepoch_int=10\n",
    "checkpoint_int=1663\n",
    "\n",
    "input_dir = 'models' + \"/\" + net_name + \"/\" + dataset_name\n",
    "if not os.path.exists(input_dir):\n",
    "    raise Exception('There is no input directory for loading network from ' + input_dir)\n",
    "\n",
    "load_name = os.path.join(input_dir,\n",
    "                         'faster_rcnn_{}_{}_{}.pth'.format(checksession_int, checkepoch_int, checkpoint_int))\n",
    "print(\"load checkpoint %s\" % (load_name))\n",
    "checkpoint = torch.load(load_name)\n",
    "fasterRCNN.load_state_dict(checkpoint['model'])\n",
    "if 'pooling_mode' in checkpoint.keys():\n",
    "    cfg.POOLING_MODE = checkpoint['pooling_mode']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load model successfully!\n"
     ]
    }
   ],
   "source": [
    "# initilize the tensor holder here.\n",
    "print('load model successfully!')\n",
    "im_data = torch.FloatTensor(1)\n",
    "query   = torch.FloatTensor(1)\n",
    "im_info = torch.FloatTensor(1)\n",
    "catgory = torch.LongTensor(1)\n",
    "gt_boxes = torch.FloatTensor(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ship to cuda\n",
    "\n",
    "cfg.CUDA = True\n",
    "fasterRCNN.cuda()\n",
    "im_data = im_data.cuda()\n",
    "query = query.cuda()\n",
    "im_info = im_info.cuda()\n",
    "catgory = catgory.cuda()\n",
    "gt_boxes = gt_boxes.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make variable\n",
    "im_data = Variable(im_data)\n",
    "query = Variable(query)\n",
    "im_info = Variable(im_info)\n",
    "catgory = Variable(catgory)\n",
    "gt_boxes = Variable(gt_boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/One-Shot-Object-Detection/output/res50/coco_2014_minival/faster_rcnn_unseen/sess1_g1_seen2_0.pkl\n",
      "Evaluating detections\n",
      "Collecting person results (1/80)\n",
      "Collecting bicycle results (2/80)\n",
      "Collecting car results (3/80)\n",
      "Collecting motorcycle results (4/80)\n",
      "Collecting airplane results (5/80)\n",
      "Collecting bus results (6/80)\n",
      "Collecting train results (7/80)\n",
      "Collecting truck results (8/80)\n",
      "Collecting boat results (9/80)\n",
      "Collecting traffic light results (10/80)\n",
      "Collecting fire hydrant results (11/80)\n",
      "Collecting stop sign results (12/80)\n",
      "Collecting parking meter results (13/80)\n",
      "Collecting bench results (14/80)\n",
      "Collecting bird results (15/80)\n",
      "Collecting cat results (16/80)\n",
      "Collecting dog results (17/80)\n",
      "Collecting horse results (18/80)\n",
      "Collecting sheep results (19/80)\n",
      "Collecting cow results (20/80)\n",
      "Collecting elephant results (21/80)\n",
      "Collecting bear results (22/80)\n",
      "Collecting zebra results (23/80)\n",
      "Collecting giraffe results (24/80)\n",
      "Collecting backpack results (25/80)\n",
      "Collecting umbrella results (26/80)\n",
      "Collecting handbag results (27/80)\n",
      "Collecting tie results (28/80)\n",
      "Collecting suitcase results (29/80)\n",
      "Collecting frisbee results (30/80)\n",
      "Collecting skis results (31/80)\n",
      "Collecting snowboard results (32/80)\n",
      "Collecting sports ball results (33/80)\n",
      "Collecting kite results (34/80)\n",
      "Collecting baseball bat results (35/80)\n",
      "Collecting baseball glove results (36/80)\n",
      "Collecting skateboard results (37/80)\n",
      "Collecting surfboard results (38/80)\n",
      "Collecting tennis racket results (39/80)\n",
      "Collecting bottle results (40/80)\n",
      "Collecting wine glass results (41/80)\n",
      "Collecting cup results (42/80)\n",
      "Collecting fork results (43/80)\n",
      "Collecting knife results (44/80)\n",
      "Collecting spoon results (45/80)\n",
      "Collecting bowl results (46/80)\n",
      "Collecting banana results (47/80)\n",
      "Collecting apple results (48/80)\n",
      "Collecting sandwich results (49/80)\n",
      "Collecting orange results (50/80)\n",
      "Collecting broccoli results (51/80)\n",
      "Collecting carrot results (52/80)\n",
      "Collecting hot dog results (53/80)\n",
      "Collecting pizza results (54/80)\n",
      "Collecting donut results (55/80)\n",
      "Collecting cake results (56/80)\n",
      "Collecting chair results (57/80)\n",
      "Collecting couch results (58/80)\n",
      "Collecting potted plant results (59/80)\n",
      "Collecting bed results (60/80)\n",
      "Collecting dining table results (61/80)\n",
      "Collecting toilet results (62/80)\n",
      "Collecting tv results (63/80)\n",
      "Collecting laptop results (64/80)\n",
      "Collecting mouse results (65/80)\n",
      "Collecting remote results (66/80)\n",
      "Collecting keyboard results (67/80)\n",
      "Collecting cell phone results (68/80)\n",
      "Collecting microwave results (69/80)\n",
      "Collecting oven results (70/80)\n",
      "Collecting toaster results (71/80)\n",
      "Collecting sink results (72/80)\n",
      "Collecting refrigerator results (73/80)\n",
      "Collecting book results (74/80)\n",
      "Collecting clock results (75/80)\n",
      "Collecting vase results (76/80)\n",
      "Collecting scissors results (77/80)\n",
      "Collecting teddy bear results (78/80)\n",
      "Collecting hair drier results (79/80)\n",
      "Collecting toothbrush results (80/80)\n",
      "Writing results json to /workspace/One-Shot-Object-Detection/output/res50/coco_2014_minival/faster_rcnn_unseen/detections_minival2014_results.json\n",
      "Loading and preparing results...     \n",
      "DONE (t=1.56s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=38.03s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=4.50s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.118\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.232\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.110\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.055\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.140\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.186\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.130\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.260\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.280\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.145\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.317\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.435\n",
      "Wrote COCO eval results to: /workspace/One-Shot-Object-Detection/output/res50/coco_2014_minival/faster_rcnn_unseen/detection_results.pkl\n",
      "test time: 53.0925s\n"
     ]
    }
   ],
   "source": [
    "average_topK=1\n",
    "\n",
    "# record time\n",
    "start = time.time()\n",
    "\n",
    "# visiualization\n",
    "vis = True\n",
    "if vis:\n",
    "    thresh = 0.05\n",
    "else:\n",
    "    thresh = 0.0\n",
    "max_per_image = 100\n",
    "\n",
    "# create output Directory\n",
    "output_dir_vu = get_output_dir(imdb_vu, 'faster_rcnn_unseen')\n",
    "\n",
    "fasterRCNN.eval()\n",
    "for avg in range(average_topK):\n",
    "    dataset_vu.query_position = avg\n",
    "    dataloader_vu = torch.utils.data.DataLoader(dataset_vu, batch_size=1,shuffle=False, num_workers=0,pin_memory=True)\n",
    "\n",
    "    data_iter_vu = iter(dataloader_vu)\n",
    "\n",
    "    # total quantity of testing images, each images include multiple detect class\n",
    "    num_images_vu = len(imdb_vu.image_index)\n",
    "    num_detect = len(ratio_index_vu[0])\n",
    "\n",
    "    all_boxes = [[[] for _ in xrange(num_images_vu)]\n",
    "                for _ in xrange(imdb_vu.num_classes)]\n",
    "\n",
    "    \n",
    "    _t = {'im_detect': time.time(), 'misc': time.time()}\n",
    "    if group_name != 0:\n",
    "      det_file = os.path.join(output_dir_vu, 'sess%d_g%d_seen%d_%d.pkl'%(checksession_int, group_name, 2, avg))\n",
    "    else:\n",
    "      det_file = os.path.join(output_dir_vu, 'sess%d_seen%d_%d.pkl'%(checksession_int, 2, avg))\n",
    "    print(det_file)\n",
    "\n",
    "    if os.path.exists(det_file):\n",
    "      with open(det_file, 'rb') as fid:\n",
    "        all_boxes = pickle.load(fid)\n",
    "    else:\n",
    "      for i,index in enumerate(ratio_index_vu[0]):\n",
    "        data = next(data_iter_vu)\n",
    "        im_data.data.resize_(data[0].size()).copy_(data[0])\n",
    "        query.data.resize_(data[1].size()).copy_(data[1])\n",
    "        im_info.data.resize_(data[2].size()).copy_(data[2])\n",
    "        gt_boxes.data.resize_(data[3].size()).copy_(data[3])\n",
    "        catgory.data.resize_(data[4].size()).copy_(data[4])\n",
    "\n",
    "\n",
    "        # Run Testing\n",
    "        det_tic = time.time()\n",
    "        rois, cls_prob, bbox_pred, \\\n",
    "        rpn_loss_cls, rpn_loss_box, \\\n",
    "        RCNN_loss_cls, _, RCNN_loss_bbox, \\\n",
    "        rois_label, weight = fasterRCNN(im_data, query, im_info, gt_boxes, catgory)\n",
    "\n",
    "\n",
    "        scores = cls_prob.data\n",
    "        boxes = rois.data[:, :, 1:5]\n",
    "\n",
    "        \n",
    "        # Apply bounding-box regression \n",
    "        if cfg.TEST.BBOX_REG:\n",
    "            # Apply bounding-box regression deltas\n",
    "            box_deltas = bbox_pred.data\n",
    "            if cfg.TRAIN.BBOX_NORMALIZE_TARGETS_PRECOMPUTED:\n",
    "            # Optionally normalize targets by a precomputed mean and stdev\n",
    "              if class_agnostic:\n",
    "                  box_deltas = box_deltas.view(-1, 4) * torch.FloatTensor(cfg.TRAIN.BBOX_NORMALIZE_STDS).cuda() \\\n",
    "                            + torch.FloatTensor(cfg.TRAIN.BBOX_NORMALIZE_MEANS).cuda()\n",
    "                  box_deltas = box_deltas.view(1, -1, 4)\n",
    "              else:\n",
    "                  box_deltas = box_deltas.view(-1, 4) * torch.FloatTensor(cfg.TRAIN.BBOX_NORMALIZE_STDS).cuda() \\\n",
    "                            + torch.FloatTensor(cfg.TRAIN.BBOX_NORMALIZE_MEANS).cuda()\n",
    "                  box_deltas = box_deltas.view(1, -1, 4 * len(imdb.classes))\n",
    "\n",
    "            pred_boxes = bbox_transform_inv(boxes, box_deltas, 1)\n",
    "            pred_boxes = clip_boxes(pred_boxes, im_info.data, 1)\n",
    "        else:\n",
    "            # Simply repeat the boxes, once for each class\n",
    "            pred_boxes = np.tile(boxes, (1, scores.shape[1]))\n",
    "\n",
    "\n",
    "        # Resize to original ratio\n",
    "        pred_boxes /= data[2][0][2].item()\n",
    "\n",
    "        # Remove batch_size dimension\n",
    "        scores = scores.squeeze()\n",
    "        pred_boxes = pred_boxes.squeeze()\n",
    "\n",
    "        # Record time\n",
    "        det_toc = time.time()\n",
    "        detect_time = det_toc - det_tic\n",
    "        misc_tic = time.time()\n",
    "\n",
    "        # Post processing\n",
    "        inds = torch.nonzero(scores>thresh).view(-1)\n",
    "        if inds.numel() > 0:\n",
    "          # remove useless indices\n",
    "          cls_scores = scores[inds]\n",
    "          cls_boxes = pred_boxes[inds, :]\n",
    "          cls_dets = torch.cat((cls_boxes, cls_scores.unsqueeze(1)), 1)\n",
    "\n",
    "          # rearrange order\n",
    "          _, order = torch.sort(cls_scores, 0, True)\n",
    "          cls_dets = cls_dets[order]\n",
    "\n",
    "          # NMS\n",
    "          keep = nms(cls_boxes[order, :], cls_scores[order], cfg.TEST.NMS)\n",
    "          cls_dets = cls_dets[keep.view(-1).long()]\n",
    "          all_boxes[catgory][index] = cls_dets.cpu().numpy()\n",
    "\n",
    "        # Limit to max_per_image detections *over all classes*\n",
    "        if max_per_image > 0:\n",
    "          try:\n",
    "            image_scores = all_boxes[catgory][index][:,-1]\n",
    "            if len(image_scores) > max_per_image:\n",
    "                image_thresh = np.sort(image_scores)[-max_per_image]\n",
    "\n",
    "                keep = np.where(all_boxes[catgory][index][:,-1] >= image_thresh)[0]\n",
    "                all_boxes[catgory][index] = all_boxes[catgory][index][keep, :]\n",
    "          except:\n",
    "            pass\n",
    "\n",
    "        misc_toc = time.time()\n",
    "        nms_time = misc_toc - misc_tic\n",
    "\n",
    "        sys.stdout.write('im_detect: {:d}/{:d} {:.3f}s {:.3f}s   \\r' \\\n",
    "            .format(i + 1, num_detect, detect_time, nms_time))\n",
    "        sys.stdout.flush()\n",
    "\n",
    "        # save test image\n",
    "        if vis and i%1==0:\n",
    "          im2show = cv2.imread(dataset_vu._roidb[dataset_vu.ratio_index[i]]['image'])\n",
    "          im2show = vis_detections(im2show, 'shot', cls_dets.cpu().numpy(), 0.8)\n",
    "\n",
    "          o_query = data[1][0].permute(1, 2,0).contiguous().cpu().numpy()\n",
    "          o_query *= [0.229, 0.224, 0.225]\n",
    "          o_query += [0.485, 0.456, 0.406]\n",
    "          o_query *= 255\n",
    "          o_query = o_query[:,:,::-1]\n",
    "\n",
    "          (h,w,c) = im2show.shape\n",
    "          o_query = cv2.resize(o_query, (h, h),interpolation=cv2.INTER_LINEAR)\n",
    "          im2show = np.concatenate((im2show, o_query), axis=1)\n",
    "\n",
    "          cv2.imwrite('./test_img/%d_d.png'%(i), im2show)\n",
    "      \n",
    "    \n",
    "      with open(det_file, 'wb') as f:\n",
    "          pickle.dump(all_boxes, f, pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "    print('Evaluating detections')\n",
    "    imdb_vu.evaluate_detections(all_boxes, output_dir_vu) \n",
    "\n",
    "\n",
    "    end = time.time()\n",
    "    print(\"test time: %0.4fs\" % (end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
